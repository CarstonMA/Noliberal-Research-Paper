---
title: "Assignment 4 - Logistic Regression"
author: "Your Name"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(caret)
library(glmnet)
library(pROC)
library(ResourceSelection)
library(dplyr)
library(tidyr)
library(ggplot2)

# Load Data
# train <- read.csv("train_data.csv") # Adjust filename
# test <- read.csv("test_data.csv") # Adjust filename

# Any additional preprocessing from assignment4.R
```

## Question 1

**Vittinghoff 5.6**  
Use the regression output in Table 5.16 and a calculation similar to that presented in (5.11) to compute the odds ratio comparing the odds of CHD in a 55-year-old individual with arcus to the corresponding odds for a 40-year-old who also has arcus.

**Answer:**

\[
OR = e^{(b_{age} \times (age_{55}-age_{40})) + (b_{arcus \times age} \times (age_{55}-age_{40}))}
\]

Here are the coefficients from the provided logistic regression model:

- \( b_{age} = 0.089647 \)
- \( b_{arcus \times age} = -0.0498298 \)


\[
OR = e^{[(0.089647 \times (55-40)) + (-0.0498298 \times (55-40))]}
\]


\[
OR = e^{[(0.089647 - 0.0498298) \times 15]}
\]

\[
OR = e^{(0.0398172 \times 15)}
\]

\[
OR = e^{0.597258}
\]

\[
OR \approx 1.817
\]

The odds ratio is approximately 1.82, indicating that a 55-year-old individual with arcus has about 1.82 times higher odds of CHD compared to a 40-year-old individual with arcus, controlling for other factors.

```{r question-1, echo=FALSE}
# Logistic regression coefficients
b_age <- 0.089647
b_arcus_age <- -0.0498298

# Difference in ages
age_diff <- 55 - 40

# Calculate odds ratio
OR <- exp((b_age + b_arcus_age) * age_diff)
OR
```

## Question 2

**Vittinghoff 5.7**  
Use the WCGS data set to fit the regression model presented in Table 5.18. Perform the Hosmer-Lemeshow goodness-of-fit test for 10, 15, 20, and 25 groups. Comment on the differences.

**Answer:**

*Model was not the same but couldn't get it closer.*

- The model appears well-calibrated for groupings of 10, 15, and 20, with p-values comfortably above the 0.05 threshold. This suggests that the predicted probabilities are consistent with observed outcomes across deciles and finer partitions.

- The lowest p-value occurs at 15 groups (p = 0.0615), which is still not statistically significant, indicating no evidence of poor model fit. At 20 and 25 groups, p-values increase again, suggesting stable calibration across finer groupings.


```{r question-2, echo=FALSE}
wcgs <- read.csv("wcgs.csv")

# Clean and prepare data to match Table 5.18
wcgs_clean <- wcgs %>%
  mutate(
    age_10 = age / 10,
    chol_50 = chol / 50,
    sbp_50 = sbp / 50,
    bmi_10 = bmi / 10,
    smoke = ifelse(smoke == "Yes", 1, 0),
    dibpat = ifelse(dibpat == "Type A", 1, 0),
    bmichol = bmi_10 * chol_50,
    bmisbp = bmi_10 * sbp_50,
    chd69 = ifelse(chd69 == "Yes", 1, 0)
  ) %>%
  select(chd69, age_10, chol_50, sbp_50, bmi_10, smoke, dibpat, bmichol, bmisbp) %>%
  na.omit()

# Fit the logistic regression model
model_wcgs <- glm(
  chd69 ~ age_10 + chol_50 + sbp_50 + bmi_10 + smoke + dibpat + bmichol + bmisbp,
  data = wcgs_clean,
  family = binomial
)

# Display model summary
summary(model_wcgs)

# Predict probabilities from model
fitted_probs <- predict(model_wcgs, type = "response")

# Hosmer-Lemeshow goodness-of-fit test
hoslem.test(wcgs_clean$chd69, fitted_probs, g = 10)
hoslem.test(wcgs_clean$chd69, fitted_probs, g = 15)
hoslem.test(wcgs_clean$chd69, fitted_probs, g = 20)
hoslem.test(wcgs_clean$chd69, fitted_probs, g = 25)
```

## Question 3

### 3(a)

Create a summary table summarizing the distributions of the variables stratified by outcome (serious infection) on the training data. Comment on what you observe.

**Answer:**

- Age, Charlson Score, and BMI appear higher on average for those with serious infections.
- Some categorical comorbidities (like CHF, DM, and RENAL FAILURE) are more prevalent in infected.
- End-of-case temperature is slightly lower in the serious infection group.


```{r question-3a, echo=FALSE}
df <- read.csv("core_temp.csv")

# serious infection defined by experiencing one of these
outcome_vars <- c("SSIDeep.fascia.",
                  "Sepsis",
                  "SSIOrganSpace",
                  "Pneumonia",
                  "Pneumonia.aspiration.",
                  "AbsessIntraAb",
                  "AbsessPelvic",
                  "Cdiff")

# remove vars that we shouldn't use for prediction
df <- df %>%
  dplyr::select(-all_of(c("DurationHosp","LOS","DEAD", 
                          outcome_vars,
                          "AnyInfection", "SuperficialInfection",
                          "FascialDehiscence", "DelayedHealing", "Infection",
                          "Sinus", "SSISuperficial.skin.", "WoundInfection")))


# change vars to categorical
num_vars <-  c("Age","BMI","CharlsonScore", "TWATemp" ,
               "LastReadingTemp",  "EndCaseTemp","SurgDuration", 
               "SeriousInfection")
cat_vars <- names(df)[!(names(df) %in% num_vars)]
df[cat_vars] <- lapply(df[cat_vars], as.factor)

# surgery type is NA so drop
df <- df %>% dplyr::select(-c(SurgeryType))
df <- na.omit(df)

# Get test train split
df$Y <- df$SeriousInfection
df <- df %>% dplyr::select(-c(SeriousInfection))
train_ids <- df$YEAR %in% c(2009, 2010)
dftrain <- df[train_ids,] %>% dplyr::select(-c(YEAR))
dftest <- df[!train_ids,] %>% dplyr::select(-c(YEAR))


# Split variables
numeric_vars <- dftrain %>% select(where(is.numeric)) %>% select(-Y) %>% names()
categorical_vars <- dftrain %>% select(where(is.factor)) %>% names()

# Summary for numeric variables
numeric_summary <- dftrain %>%
  group_by(Y) %>%
  summarise(across(all_of(numeric_vars),
                   list(mean = ~mean(.x, na.rm = TRUE),
                        sd = ~sd(.x, na.rm = TRUE)),
                   .names = "{.col}_{.fn}"),
            .groups = "drop") %>%
  pivot_longer(-Y, names_to = c("Variable", ".value"), names_sep = "_")

# Summary for categorical variables
categorical_summary <- dftrain %>%
  select(Y, all_of(categorical_vars)) %>%
  pivot_longer(-Y, names_to = "Variable", values_to = "Value") %>%
  group_by(Y, Variable, Value) %>%
  summarise(Count = n(), .groups = "drop") %>%
  group_by(Variable, Y) %>%
  mutate(Proportion = Count / sum(Count)) %>%
  ungroup()

# Output both tables
list(
  NumericSummary = numeric_summary,
  CategoricalSummary = categorical_summary
)
```

### 3(b)

Fit an initial model (Model-LR) that contains all variables as predictors.

**Answer:**

```{r question-3b, echo=FALSE}
# Fit logistic regression using all predictors
model_lr <- glm(Y ~ ., data = dftrain, family = binomial)
model_lr_summary <- summary(model_lr)
print(model_lr_summary)
```

### 3(c)

Fit a lasso and ridge regression model (Model-Lasso and Model-Ridge) using 5-fold cross-validation.

**Answer:**

```{r question-3c, echo=FALSE}
# Prepare model matrix
x <- model.matrix(Y ~ ., data = dftrain)[, -1]
y <- dftrain$Y

# Fit Lasso
cv_lasso <- cv.glmnet(x, y, family = "binomial", alpha = 1, nfolds = 5)
model_lasso <- cv_lasso$glmnet.fit
lambda_lasso <- cv_lasso$lambda.min
coef_lasso <- coef(model_lasso, s = lambda_lasso)

# Fit Ridge
cv_ridge <- cv.glmnet(x, y, family = "binomial", alpha = 0, nfolds = 5)
model_ridge <- cv_ridge$glmnet.fit
lambda_ridge <- cv_ridge$lambda.min
coef_ridge <- coef(model_ridge, s = lambda_ridge)

coef_lasso_df <- as.data.frame(as.matrix(coef_lasso)) %>%
  tibble::rownames_to_column("Variable") %>%
  rename(Lasso_Coefficient = 2) 

coef_ridge_df <- as.data.frame(as.matrix(coef_ridge)) %>%
  tibble::rownames_to_column("Variable") %>%
  rename(Ridge_Coefficient = 2)  

coef_comparison <- full_join(coef_lasso_df, coef_ridge_df, by = "Variable")
coef_comparison
```

## Question 4

### 4(a)

Interpret the estimated coefficients for the intercept, one categorical variable, and one continuous variable in the logistic regression model Model-LR.

**Answer:**

- **Intercept (Estimate = 0.6898):**  
  This is the log-odds of a serious infection when all predictors are zero. Since zero is not meaningful for all variables, the intercept is not directly interpretable.

- **OBESE1 (Estimate = -0.9823):**  
  This coefficient compares patients who are obese (OBESE1 = 1) to those who are not (OBESE1 = 0). The negative value indicates that obesity is associated with lower odds of serious infection, holding all other variables constant. 
  
  The odds ratio is:  
  \[
  e^{-0.9823} \approx 0.374
  \]  
  This means obese patients have 63% lower odds of serious infection compared to non-obese patients, all else equal.

- **BMI (Estimate = 0.0392):**  
  BMI is a continuous predictor. Each 1-unit increase in BMI is associated with a  
  \[
  e^{0.0392} \approx 1.04
  \]  
  or about a 4% increase in the odds of serious infection, holding other variables constant. This indicates that although obesity as a category reduces odds, increasing BMI overall is still associated with a slightly increased risk.

### 4(b)

Create a table of the estimated coefficients for all three models and comment on the differences observed. Explain why you observe any differences.

**Answer:**

- **Lasso regression** shrinks some coefficients exactly to zero (`CharlsonScore`, `CHF1`, `RENLFAIL1`), effectively performing variable selection. This reduces model complexity and may improve interpretability.
- **Ridge regression** shrinks coefficients toward zero but does not eliminate variables.
- The standard logistic regression (Model-LR) allows full coefficient flexibility without regularization, which can lead to overfitting.

```{r question-4b, echo=FALSE}
lr_coefs <- coef(model_lr)
lr_df <- tibble::tibble(
  Variable = names(lr_coefs),
  Model_LR_Coefficient = as.numeric(lr_coefs)
)

coef_comparison <- lr_df %>%
  full_join(coef_lasso_df, by = "Variable") %>%
  full_join(coef_ridge_df, by = "Variable") %>%
  dplyr::filter(Variable %in% c("(Intercept)", "Age", "FEMALE1", "BMI", "CharlsonScore", "CHF1", "VALVE1", "DM1", "RENLFAIL1", "LIVER1"))

# Display comparison table
knitr::kable(coef_comparison, digits = 4, caption = "Comparison of Coefficients Across Models")
```

## Question 5

### 5(a)

Create an ROC curve plot for all three models and find the corresponding AUCs. Comment on the results. Pick the model with the highest AUC.

**Answer:**

The Logistic Regression model has the highest AUC. This suggests that despite regularization benefits, the unpenalized model provided slightly better discrimination for this dataset.

```{r question-5a, echo=FALSE}
library(pROC)

# Predictions
pred_lr <- predict(model_lr, type = "response")

x <- model.matrix(Y ~ ., data = dftrain)[, -1]
pred_lasso <- predict(model_lasso, newx = x, s = lambda_lasso, type = "response")
pred_ridge <- predict(model_ridge, newx = x, s = lambda_ridge, type = "response")

# AUCs
roc_lr <- suppressMessages(roc(dftrain$Y, pred_lr))
roc_lasso <- suppressMessages(roc(dftrain$Y, as.numeric(pred_lasso)))
roc_ridge <- suppressMessages(roc(dftrain$Y, as.numeric(pred_ridge)))

# Plot
plot(roc_lr, col = "blue", main = "ROC Curves for All Models")
lines(roc_lasso, col = "red")
lines(roc_ridge, col = "darkgreen")
legend("bottomright", legend = c("Logistic Regression", "Lasso", "Ridge"),
       col = c("blue", "red", "darkgreen"), lty = 1)

# Store AUCs
auc_values <- tibble::tibble(
  Model = c("Logistic Regression", "Lasso", "Ridge"),
  AUC = c(auc(roc_lr), auc(roc_lasso), auc(roc_ridge))
)

auc_values
```

### 5(b)

Create a calibration plot for your model and perform a goodness-of-fit test. Comment fully on the results.

**Answer:**

The calibration plot demonstrates that the predicted probabilities from the logistic regression model are well-aligned with the observed proportions of serious infection. Most of the points lie close to the 45-degree reference line, indicating that predicted risks correspond closely to actual event rates across deciles.

The Hosmer-Lemeshow test further confirms this impression:

- **Chi-square statistic**: 8.35  
- **Degrees of freedom**: 8  
- **p-value**: 0.3999

This p-value is well above the conventional 0.05 threshold, meaning we fail to reject the null hypothesis that the model fits well. There is no statistical evidence of miscalibration.

- A good calibration means that if the model predicts a 10% chance of infection, approximately 10 out of 100 such patients actually do get an infection.
- The combination of visual and statistical evidence here supports the reliability of the model’s predicted probabilities.


```{r question-5b, echo=FALSE}
# Get predicted probabilities
pred_probs <- predict(model_lr, type = "response")

# Calibration plot data (10 bins)
calib_df <- dftrain %>%
  mutate(pred_prob = pred_probs,
         bin = ntile(pred_prob, 10)) %>%
  group_by(bin) %>%
  summarise(
    mean_pred = mean(pred_prob),
    obs_rate = mean(Y),
    n = n()
  )

# Plot
ggplot(calib_df, aes(x = mean_pred, y = obs_rate)) +
  geom_point(size = 2) +
  geom_line() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  labs(x = "Mean Predicted Probability",
       y = "Observed Proportion of Serious Infection",
       title = "Calibration Plot: Logistic Regression Model") +
  theme_minimal()

# Hosmer-Lemeshow test
hl_test <- hoslem.test(dftrain$Y, pred_probs, g = 10)
hl_test
```

### 5(c)

Choose a threshold probability to predict class 1 and explain why you chose this threshold. Then create a confusion matrix. Comment on accuracy, sensitivity, and specificity.

**Answer:**

We choose a threshold of 0.15 to classify predicted probabilities into binary outcomes. This lower threshold reflects a preference for higher sensitivity — meaning we would rather flag more potential serious infections, even at the expense of some false positives.

- Accuracy is 90.7%, which appears high at first glance. However, given the low prevalence of class 1, this metric is somewhat misleading. The model mostly classifies negatives correctly, which inflates the accuracy even if it performs poorly at identifying positives.

- Sensitivity is 25.6%, meaning the model only correctly identifies about one-quarter of the actual positive cases. This low sensitivity suggests that many true cases of class 1 are being missed.

- Specificity is 94.6%, indicating that the model does a very good job of correctly identifying negative cases. This means that most people without the condition are correctly classified.


```{r question-5c, echo=FALSE}
# Predicted probabilities
pred_probs <- predict(model_lr, type = "response")

# Choose threshold
threshold <- 0.15
pred_class <- ifelse(pred_probs > threshold, 1, 0)

# Confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(dftrain$Y), positive = "1")
conf_matrix
```

## Question 6

### 6(a)

Create a summary table for the test data. How are the observed distributions different from the training data?

**Answer:**

- Charlson score is higher in infected patients in the test set than in training.
- Surgical duration remains predictive so longer surgeries are consistently associated with higher infection rates.
- Age is slightly lower in the test set overall, suggesting a modest demographic shift.
- Temperature variables remain relatively stable, though test data shows slightly higher end case temperatures.

#### Categorical Summary:

- Distributions for binary comorbidities are broadly similar between sets.
- Anemia: 24% prevalence in test set vs. ~19% in training.
- Coagulation disorders and diabetes are slightly more prevalent in the test set.

**Conclusion:**

While the distributions are generally similar, the test set shows:
- A modest increase in comorbidity severity (e.g., Charlson, ANEMDEF),
- Slight demographic differences (age),
- Consistent trends in key predictors (surgery length, temperatures).

```{r question-6a, echo=FALSE}
# Summary for numeric variables in test set
numeric_summary_test <- dftest %>%
  group_by(Y) %>%
  summarise(across(all_of(numeric_vars),
                   list(mean = ~mean(.x, na.rm = TRUE),
                        sd = ~sd(.x, na.rm = TRUE)),
                   .names = "{.col}_{.fn}"),
            .groups = "drop") %>%
  pivot_longer(-Y, names_to = c("Variable", ".value"), names_sep = "_")

# Summary for categorical variables in test set
categorical_summary_test <- dftest %>%
  select(Y, all_of(categorical_vars)) %>%
  pivot_longer(-Y, names_to = "Variable", values_to = "Value") %>%
  group_by(Y, Variable, Value) %>%
  summarise(Count = n(), .groups = "drop") %>%
  group_by(Variable, Y) %>%
  mutate(Proportion = Count / sum(Count)) %>%
  ungroup()

# Output both tables
list(
  NumericSummary_Test = numeric_summary_test,
  CategoricalSummary_Test = categorical_summary_test
)
```

### 6(b)

Find the confusion matrix and AUC value for the chosen model on the validation data. Comment on your observations.

**Answer:**

- **Accuracy**: 85.4%  
- **Sensitivity**: 19.1%  
- **Specificity**: 92.5%  
- **AUC**: 0.610


- The AUC dropped from 0.739 to 0.610 — a notable decrease, suggesting the model does not generalize perfectly and may be overfitted to the training data.
- **Sensitivity (19.1% is low, meaning the model missed most serious infections despite a low threshold of 0.15.
- Specificity remains high, meaning the model correctly identified most non-infections.
- The positive predictive value 21.5% and negative predictive value 91.4% reflect the class imbalance, as serious infections are rare.
-While the model retains good specificity, its discriminatory power and sensitivity have dropped in the validation set, indicating a limited ability to detect serious infections in unseen data.
- The model is not strongly generalizable across time or patient populations.

```{r question-6b, echo=FALSE}
# Predict probabilities on test data
pred_test_probs <- predict(model_lr, newdata = dftest, type = "response")

# Apply threshold
threshold <- 0.15
pred_test_class <- ifelse(pred_test_probs > threshold, 1, 0)

# Confusion matrix
conf_matrix_test <- caret::confusionMatrix(as.factor(pred_test_class), as.factor(dftest$Y), positive = "1")

# AUC
roc_test <- pROC::roc(dftest$Y, pred_test_probs)
auc_test <- pROC::auc(roc_test)

list(
  ConfusionMatrix = conf_matrix_test,
  AUC = auc_test
)
```

### 6(c)

Overall, what do the results indicate about the model’s ability to generalize to populations outside of the training population?

**Answer:**

The results suggest that while the logistic regression model performs reasonably well on the training data, its ability to generalize to new, unseen populations is limited.

- AUC dropped from 0.739 to 0.610 — a significant reduction in discriminatory ability.
- Sensitivity on the test set was only 19.1%, even using a low threshold (0.15), indicating the model misses a majority of true serious infections.
- Specificity remained high, showing the model correctly classifies most non-infected cases.
- The confusion matrix revealed the model is overly conservative in predicting positive cases.
- The drop in AUC and sensitivity highlights that the model likely overfit the training data.
- Differences in comorbidities and patient characteristics between training and validation set may have contributed to reduced performance.

---

# Appendix: R Code



```{r appendix, ref.label = c("question-1", "question-2", "question-3a", "question-3b", "question-3c", "question-4a", "question-4b", "question-5a", "question-5b", "question-5c", "question-6a", "question-6b"), echo = TRUE, eval = FALSE}
```

---
